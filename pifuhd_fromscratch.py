# -*- coding: utf-8 -*-
"""PIFuHD_fromscratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VUDf2oyMT-mMMxeaerFy5pUR-WKzdTnn
"""

from PIL import Image

"""Preprocessing

"""

image = Image.open('/content/Kevin.jpg')
lowres_kev = image.resize((128, 128))
lowres_kev.save('lowres_kev.jpg')

image = Image.open('/content/Kevin.jpg')
highres_kev = image.resize((512, 512))
highres_kev.save('highres_kev.jpg')

"""Coarse Model

"""

# Use a pre-trained model or build a neural network for the Coarse Module
# coarse_model = load_coarse_model()

# # Encode low-resolution feature embeddings to obtain 3D embeddings
# coarse_embeddings = coarse_model.encode(low_res_images)

!pip install tensorflow

"""Coarse Model


1.   Instead of 6 for num classes which is basically the number of avatar classes we need to change
2.   List item


"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
import numpy as np

# Create a simple coarse module architecture
def create_coarse_model(input_shape):
    input_layer = Input(shape=input_shape)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = GlobalAveragePooling2D()(x)  # Global average pooling for feature aggregation
    coarse_output = Dense(256, activation='relu')(x)  # Adjust output dimension as needed

    coarse_model = Model(inputs=input_layer, outputs=coarse_output)
    return coarse_model

# Define the input shape of your images
input_shape = (128, 128, 3)  # Adjust dimensions as needed

# Create the coarse model
coarse_model = create_coarse_model(input_shape)

# Load a low-resolution image for encoding
low_res_image = tf.keras.preprocessing.image.load_img('/content/lowres_kev.jpg', target_size=(128, 128))
low_res_image = tf.keras.preprocessing.image.img_to_array(low_res_image)
low_res_image = np.expand_dims(low_res_image, axis=0)
low_res_image /= 255.0  # Normalize pixel values

# Encode the low-resolution image to obtain feature embeddings
coarse_embeddings = coarse_model.predict(low_res_image)

print("Coarse embeddings shape:", coarse_embeddings.shape)

"""Fine Module

"""

# # Use a pre-trained model or build a neural network for the Fine Module
# fine_model = load_fine_model()

# # Encode high-resolution image features
# high_res_features = fine_model.encode(high_res_images)

# # Combine high-resolution features with Coarse Module 3D embeddings
# combined_features = combine_features(high_res_features, coarse_embeddings)

# # Predict occupancy probability field using combined features
# occupancy_field = fine_model.predict_occupancy(combined_features)

# Create Fine Module
fine_model = FineModule(fine_input_shape)

# Load high-resolution image for encoding
high_res_image = tf.keras.preprocessing.image.load_img('/content/highres_kev.jpg', target_size=(512, 512))
high_res_image = tf.keras.preprocessing.image.img_to_array(high_res_image)
high_res_image = np.expand_dims(high_res_image, axis=0)
high_res_image /= 255.0  # Normalize pixel values

# Encode high-resolution image features
high_res_features = fine_model.encode(high_res_image)

# Combine high-resolution features with Coarse Module 3D embeddings
combined_features = combine_features(high_res_features, coarse_embeddings)

# Predict occupancy probability field using combined features
occupancy_field = fine_model.predict_occupancy(combined_features)

"""Normal Map Prediction"""

# Predict normal maps for the front and back sides in image space
front_normal_map = fine_model.predict_normal_map(high_res_images, 'front')
back_normal_map = fine_model.predict_normal_map(high_res_images, 'back')